# 配置

## 同步时间

```bash
# crontab -e

0 1 * * * /usr/sbin/ntpdate cn.pool.ntp.org

//分 小时 日 月 每周的第几天 命令

# vi /etc/sysconfig/clock

ZONE="Asia/Shanghai"
连接到上海时区文件
# rm /etc/localtime
# ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
# reboot

（3）手动同步时间
直接在Terminal运行下面的命令：
[root@localhost hfut]# /usr/sbin/ntpdate cn.pool.ntp.org
```



## 配置网络

```bash
# vi /etc/sysconfig/network-scripts/ifcfg-ens32
```

内容为

```bash
（1）bootproto=static
（2）onboot=yes
（3）在最后加上几行，IP地址、子网掩码、网关、dns服务器
IPADDR=192.168.79.129
NETMASK=255.255.255.0
GATEWAY=192.168.79.2
DNS1=192.168.79.2
```

## 配置主机名映射

```bash
# vi /etc/hosts
```

在下面追加

```bash
192.168.79.129  master
192.168.79.130  slave1
192.168.79.131  slave2
```

## 配置主机名

```bash
# vi /etc/sysconfig/network
```

```bash
NETWORKING=yes
HOSTNAME=master	[slave1]	[slave2]
```

此时已经配置好了ip地址和主机名，安装包全部都在master节点里面

还没有配置主机名映射，需要去配置/etc/hosts

# 从这开始

# Hadoop配置

1、配置免密登陆

```bash
$ ssh-keygen -t rsa

$ ll ~/.ssh

$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

$ chmod 600 ~/.ssh/authorized_keys

$ scp ~/.ssh/authorized_keys liujiawei@slave1:~/.ssh
$ scp ~/.ssh/authorized_keys liujiawei@slave2:~/.ssh
```

2、配置Java（三台机子）



3、配置Hadoop

**hadoop-env.sh**

```sh
export JAVA_HOME=/home/liujiawei/app/jdk1.8.0_171
```

将`JAVA_HOME`修改为本地的JAVA_HOME位置

**yarn-env.sh**

```sh
# export JAVA_HOME=/home/y/libexec/jdk1.6.0/
```

去掉#号，将`JAVA_HOME`改为自己的JAVA_HOME

**core-site.xml**

```xml
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://master:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/liujiawei/app/hadoop-2.6.0/hadoopdata</value>
</property>
</configuration>
```

`hadoop.tmp.dir`的value设置为自己创建的Hadoop数据的仓库位置

**hdfs-site.xml**

```xml
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
</configuration>
```

**yarn-site.xml**

```xml
<?xml version="1.0"?>
<configuration>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>master:18040</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>master:18030</value>
</property>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>master:18025</value>
</property>
<property>
<name>yarn.resourcemanager.admin.address</name>
<value>master:18141</value>
</property>
<property>
<name>yarn.resourcemanager.webapp.address</name>
<value>master:18088</value>
</property>
</configuration>
```

**mapred-site.xml**

```xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>
```

**slaves**

```shell
slave1
slave2
```

**将Hadoop文件分发到集群**

```bash
$ scp -r hadoop-2.6.0/ liujiawei@slave1:~/app
$ scp -r hadoop-2.6.0/ liujiawei@slave2:~/app

$ source ~/.bash_profile
```

**格式化hdfs**

```bash
$ hadoop namenode -format
```

**检查**

```bash
$ jps
#master
16208 NameNode
16776 Jps
16377 SecondaryNameNode
16522 ResourceManager

#slave
16007 DataNode
16109 NodeManager
16205 Jps

# 端口查看	 http://master:18088/			http://master:50070/
```



# 安装mysql

```bash
# yum -y install perl
# rpm -qa | grep mar
# rpm -e --nodeps mariadb-libs-5.5.60-1.el7_5.x86_64
# rpm -e --nodeps libsmartcols-2.23.2-59.el7.x86_64
# rpm -ivh MySQL-client-5.1.73-1.glibc23.x86_64.rpm
# rpm -ivh MySQL-server-5.1.73-1.glibc23.x86_64.rpm
```

然后按照提示修改数据库密码

# 安装hive

**1、分配权限**

```bash
# service mysqld start

# mysql -uroot -p258079

mysql> grant all on *.* to hadoop@'%' identified by 'hadoop';
mysql> grant all on *.* to hadoop@'localhost' identified by 'hadoop';
mysql> grant all on *.* to hadoop@'master' identified by 'hadoop';

创建数据库：
mysql> create database hive;

查看数据库：
mysql> show databases;

退出MySQL Shell：
mysql> quit;
```

**2、配置hive**

```bash
# cd ~/app/apache-hive-1.2.1-bin/conf/
# vi hive-site.xml
```

输入下列配置内容

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
<name>hive.metastore.local</name>
<value>true</value>
</property>
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://master:3306/hive?characterEncoding=UTF-8</value>
</property>
<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
</property>
<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>hadoop</value>
</property>
<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>hadoop</value>
</property>
</configuration>
```

**3、将`mysql-connector-jar`放入`hive/bin`下**

**4、将hive/bin添加至环境变量**

**5、hive/bin/jline-2.12.jar移动至hadoop/share/hadoop/yarn/lib下，并将原来的jline-0.9.94.jar删除**

**6、启动hive**

```bash
# hive
```



# 安装Python

**1、解压tar**

```bash
$ tar -zxvf python-3.6.3.tgz
```

**2、建立python安装目录并配置&&编译**

```bash
$ mkdir python3
$ cd Python-3.6.3/

# 安装编译所需依赖
# yum install zlib-devel bzip2-devel readline-devel.x86_64 openssl-devel ncurses-devel gcc-c++ gcc xz -y

$ ./configure --prefix .../python3

$ make -j 9 && make install
```

**3、建立软链接**

```bash
# cd/usr/bin
# mv python python_back(#)
# ln -s ../python3.6 /usr/bin/python
# ln -s ../pip3 /usr/bin/pip3

# 一定要写绝对路径！
```



**#换源**

```bash
$ cd ~
$ mkdir .pip
$ vi pip.conf

[global]
index-url = https://mirrors.aliyun.com/pypi/simple/
```



# 安装HBase

**1、配置hbase-env.sh**，修改JAVA_HOME



**2、配置hbase-site.xml**

```xml
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<property>
<name>hbase.cluster.distributed</name>
<value>true</value>
</property>
<property>
<name>hbase.rootdir</name>
<value>hdfs://master:9000/hbase</value>
</property>
<property>
<name>hbase.zookeeper.quorum</name>
<value>master</value>
</property>
<property>
<name>hbase.master.info.port</name>
<value>60010</value>
</property>
</configuration>
```



**3、regionservers**

```bash
slave1
slave2
```



# 安装Spark

修改conf/spark-env.sh.template

```bash
$ cp spark-env.sh.template spark-env.sh
```

添加以下内容

```bash
export HADOOP_HOME=/home/liujiawei/app/hadoop-2.6.0
export JAVA_HOME=/home/liujiawei/app/jdk1.8.0_171  
export SPARK_MASTER_IP=master
export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8080
```

修改conf/slaves.template

```bash
cp slaves.template slaves
```

删除localhost，添加

```bash
master
slave1
slave2
```

分发到集群

```bash
$ scp -r ~/app/spark-1.6.0-bin-hadoop2.6/ liujiawei@slave1:~/app
$ scp -r ~/app/spark-1.6.0-bin-hadoop2.6/ liujiawei@slave2:~/app
```

配置环境变量

```bash
export SPARK_HOME=/home/liujiawei/app/spark-1.6.0-bin-hadoop2.6
export PATH=$SPARK_HOME/bin:$PATH
```

启动spark

```bash
$ ~/.../sbin/start-all.sh
```

